<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.16.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>yolo_ros: YOLO for ROS 2: Perception stack for YOLOv9 + ROS2 cone detection (Camera only)</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">yolo_ros: YOLO for ROS 2
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.16.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search/",'.html');
</script>
<script type="text/javascript">
$(function() { codefold.init(); });
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search',false);
  $(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Perception stack for YOLOv9 + ROS2 cone detection (Camera only) </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a></p>
<p>Camera-based cone detection pipeline for Formula Student Autonomous using <b>YOLOv9 (ONNX)</b> integrated with <b>ROS2</b>.</p>
<p>This repository provides:</p>
<ul>
<li>YOLOv9 training workflow</li>
<li>ONNX export pipeline</li>
<li>ROS2 integration</li>
<li>Real-time cone detection for FS environments</li>
</ul>
<hr  />
<h1 class="doxsection"><a class="anchor" id="autotoc_md2"></a>
System Overview</h1>
<p>The perception pipeline is structured as:</p>
<p>usb_cam (camera) ↓ YOLO ROS2 Node (image resize → ONNX inference) ↓ Detection messages ↓ RViz / downstream planning modules</p>
<p>The YOLO node resizes incoming frames to the model input size before inference.</p>
<hr  />
<h1 class="doxsection"><a class="anchor" id="autotoc_md4"></a>
Requirements</h1>
<h2 class="doxsection"><a class="anchor" id="autotoc_md5"></a>
Hardware &amp; Software</h2>
<ul>
<li><b>Framework:</b> ROS 2 (Humble/Iron/Jazzy)</li>
<li><b>Camera:</b> Luxonis OAK-D LR (DepthAI)</li>
<li><b>GPU:</b> NVIDIA (CUDA enabled)</li>
<li><b>Model:</b> Custom trained YOLOv9 (<span class="tt">.onnx</span> format for TensorRT acceleration)</li>
</ul>
<p>Install YOLO:</p>
<div class="fragment"><div class="line"> bash</div>
<div class="line">pip install ultralytics</div>
</div><!-- fragment --><p>Install ONNX Runtime:</p>
<div class="fragment"><div class="line"> bash</div>
<div class="line">pip install onnx onnxruntime</div>
</div><!-- fragment --><hr  />
<h1 class="doxsection"><a class="anchor" id="autotoc_md7"></a>
Export to ONNX</h1>
<p>Export the trained model to ONNX format:</p>
<div class="fragment"><div class="line"> bash</div>
<div class="line">yolo export model=best.pt format=onnx imgsz=[640,640]</div>
</div><!-- fragment --><p>This produces a static 640×640 ONNX model.</p>
<hr  />
<h1 class="doxsection"><a class="anchor" id="autotoc_md9"></a>
ROS2 Setup</h1>
<h2 class="doxsection"><a class="anchor" id="autotoc_md10"></a>
Start Camera</h2>
<p><b>For webcam:</b> </p><div class="fragment"><div class="line"> bash</div>
<div class="line">ros2 run usb_cam usb_cam_node_exe</div>
</div><!-- fragment --><p><b>For Physical OAK-D Camera:</b> </p><div class="fragment"><div class="line">ros2 launch depthai_ros_driver camera.launch.py</div>
</div><!-- fragment --><p><b>For EUFS Simulator:</b> Launch the simulator environment as per the EUFS documentation. The camera topic will typically be <span class="tt">/zed/left/image_rect_color</span>.</p>
<p><em>Note: The EUFS Simulator and OAK-D publish images using <b>Best Effort (Reliability=2)</b> QoS. The YOLO node must match this, or the nodes will not communicate.</em></p>
<div class="fragment"><div class="line">ros2 launch yolo_bringup yolov9.launch.py \</div>
<div class="line">  model:=/path/to/your/best.onnx \</div>
<div class="line">  device:=cuda:0 \</div>
<div class="line">  input_image_topic:=/oak/rgb/preview/image_raw \</div>
<div class="line">  reliability:=2</div>
</div><!-- fragment --><p><b>Always ensure <span class="tt">/image_raw</span> is publishing:</b></p>
<div class="fragment"><div class="line"> bash</div>
<div class="line">ros2 topic list</div>
<div class="line">ros2 topic hz /image_raw</div>
</div><!-- fragment --><hr  />
<h2 class="doxsection"><a class="anchor" id="autotoc_md12"></a>
Launch YOLO Node</h2>
<div class="fragment"><div class="line"> bash</div>
<div class="line">ros2 launch yolo_bringup yolov9.launch.py   imgsz_height:=640   imgsz_width:=640  model:=/path/to/your/best.onnx device:=cuda:0</div>
</div><!-- fragment --><h3 class="doxsection"><a class="anchor" id="autotoc_md13"></a>
Parameters</h3>
<p>Parameter Description </p><hr  />
<p> <span class="tt">imgsz_height</span> Model input height <span class="tt">imgsz_width</span> Model input width <span class="tt">device</span> CPU or CUDA usage (GPU first) <span class="tt">model</span> weights file used <span class="tt">threshold</span> Confidence threshold (adjustable if necessary)</p>
<hr  />
<h1 class="doxsection"><a class="anchor" id="autotoc_md15"></a>
Viewing in RViz</h1>
<p>Launch RViz:</p>
<div class="fragment"><div class="line"> bash</div>
<div class="line">rviz2</div>
</div><!-- fragment --><p>Add: - Image display (<span class="tt">/image_raw</span>) - Detection topic (e.g. <span class="tt">/yolo/detections</span>)</p>
<p>Ensure the correct image topic is selected.</p>
<hr  />
<h1 class="doxsection"><a class="anchor" id="autotoc_md17"></a>
Configuration Notes</h1>
<ul>
<li>The ONNX model is exported with a fixed input size (640×640).</li>
<li>The ROS2 node resizes incoming images before inference.</li>
<li>Adjust <span class="tt">threshold</span> and <span class="tt">iou</span> to balance precision and recall.</li>
<li>Ensure camera resolution is compatible with model input settings.</li>
</ul>
<hr  />
<h1 class="doxsection"><a class="anchor" id="autotoc_md19"></a>
Extending the Pipeline</h1>
<p>The detection output can be integrated with:</p>
<ul>
<li>LiDAR clustering</li>
<li>Sensor fusion modules</li>
<li>Path planning systems</li>
<li>Autonomous driving logic</li>
</ul>
<p>The camera-only pipeline operates independently and can be used as a standalone perception module.</p>
<hr  />
<h1 class="doxsection"><a class="anchor" id="autotoc_md21"></a>
Best Practices</h1>
<ul>
<li>Always export from <span class="tt">best.pt</span></li>
<li>Keep a backup of trained weights</li>
<li>Verify ONNX model loads correctly before deployment</li>
<li>Confirm ROS topics are publishing before debugging inference</li>
</ul>
<h1 class="doxsection"><a class="anchor" id="autotoc_md22"></a>
Known Issues</h1>
<h3 class="doxsection"><a class="anchor" id="autotoc_md23"></a>
1. Camera Permissions (udev rules)</h3>
<p>If you get an <span class="tt">X_LINK_INSUFFICIENT_PERMISSIONS</span> error when launching the OAK-D, Linux is blocking USB access to the Movidius chip. Run this once: <span class="tt"> </span> <span class="tt">bash
echo 'SUBSYSTEM=="usb", ATTRS{idVendor}=="03e7", MODE="0666"' | sudo tee /etc/udev/rules.d/80-movidius.rules
sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger
</span> <span class="tt"> </span> <em>(Unplug and re-plug the camera after running this).</em></p>
<h3 class="doxsection"><a class="anchor" id="autotoc_md24"></a>
2. Python Dependencies (The NumPy Trap)</h3>
<p>ROS 2 requires an older version of NumPy, but Ultralytics (YOLO) requires a newer one. To prevent <span class="tt">AttributeError</span> crashes, force this specific version range: <span class="tt"> </span> <span class="tt">bash
pip3 install "numpy&gt;=1.26.4,&lt;2.0.0"
</span> <span class="tt"> </span> </p>
</div></div><!-- PageDoc -->
<a href="doxygen_crawl.html"></a>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.16.1
</small></address>
</div><!-- doc-content -->
</body>
</html>
